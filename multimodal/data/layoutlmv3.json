{
    "Title": "LayoutLMv3: Pre-training for Document AI",
    "Authors": "with Unified Text and Image Masking\n\nYupan Huang\n\nSun Yat-sen Universityhuangyp28@mail2.sysu.edu.cn\n\nTengchao Lv\n\nMicrosoft Research Asiatengchaolv@microsoft.com\n\nLei Cui\n\nMicrosoft Research Asialecu@microsoft.com\n\nYutong Lu\n\nSun Yat-sen Universityluyutong@mail.sysu.edu.cn\n\nFuru Wei\n\nMicrosoft Research Asiafuwei@microsoft.com",
    "Abstract": "Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose **LayoutLMv3** to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3.",
    "Key Findings": "In this paper, we present LayoutLMv3 to pre-train the multimodal Transformer for Document AI, which redesigns the model architecture and pre-training objectives for LayoutLM. Distinguishing from the existing multimodal model in Document AI, LayoutLMv3 does not rely on a pre-trained CNN or Faster R-CNN backbone to extract visual features, significantly saving parameters and eliminating region annotations. We use unified text and image masking pre-training objectives: masked language modeling, masked image modeling, and word-patch alignment, to learn multimodal representations. Extensive experimental results have demonstrated the generality and superiority of LayoutLMv3 for both text-centric and image-centric Document AI tasks with the simple architecture and unified objectives. In future research, we will investigate scaling up pre-trained models so that the models can leverage more training data to drive SOTA results further. In addition, we will explore few-shot and zero-shot learning capabilities to facilitate more real-world business scenarios in the Document AI industry.",
    "References": [
        "(1)",
        "Ali et al. (2021) Alsadedia Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. 2021. Xcit: Cross-covariance image transformers. In _NeurIPS_.",
        "Papalaynayn et al. (2021) Srikar Papalaynayn, Bhavana Jasni, Bhavana Uzuka Kota, Yuchung Xie, and R. Manmatha. 2021. Deformer End-to-end Transformer for Document Understanding. In _ICCV_.",
        "Bao et al. (2022) Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. 2022. BERT: BERT Pre-Training of Image Transformers. In _ICLR_.",
        "Cai and Vasconcelos (2018) Zhaowei Cai and Nuno Vasconcelos. 2018. Cascade r-cnn: Delving into high quality object detection. In _CVPR_.",
        "Chen et al. (2020) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In _ECCV_.",
        "Cho et al. (2020) Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, and Aniruddha Kembhavi. 2020. X-IXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers. In _EMNLP_.",
        "Conneau et al. (2020) Alexis Conneau, Karlicky Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In _ACL_.",
        "Cui et al. (2021) Lei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. 2021. Document AI: Benchmarks, Models and Applications. _arXiv preprint arXiv:2111.08069_ (2021).",
        "Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenneth Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _NAACL_.",
        "Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. 2021. Cogview: Mastering text-to-image generation via transformers. In _NeurIPS_.",
        "Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolosnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houbly. 2021. An Image in North 16x6 Words Transformers for Image Recognition at Scale. In _ICLR_.",
        "Dou et al. (2021) Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuobang Wang, Lijuan Wang, Chengguang Zhu, Zicheng Liu, Michael Zeng, et al. 2021. An Empirical Study of Training End-to-End Vision-and-Language Transformers. _arXiv preprint arXiv:2111.02387_ (2021).",
        "Garracek et al. (2021) Lukasz Garracek, Brafil Powalkis, Tomas Stanislawek, Bartosz Topolski, Piotr Halama, Michal Turski, and Filip Gralinski. 2021. LAMBERT: Layout-Aware Language Modeling for Information Extraction. In _ICDAR_.",
        "Gu et al. (2021) Jiuxiang Gu, Jason Kuei, Vlad Morariu, Handong Zhao, Rajiv Jain, Nikolaos Rampalios, Ani Nenkova, and Tong Sun. 2021. UniDoc: Unified Pretraining Framework for Document Understanding. In _NeurIPS_.",
        "Gu et al. (2022) Zhangguan Gu, Changhang Meng, Ke Wang, Jian Lan, Weiqiang Wang, Ming Gu, and Liqing Zhang. 2022. XLTayanCutL: Towards Layout-Aware Multimodal Networks For Visually-Etc Document Understanding. In _CVPR_.",
        "Hufey et al. (2015) Adam W Hufey, Alex Uffes, and Konstantinos G Derpanis. 2015. Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval. In _ICDAR_.",
        "Hong et al. (2022) Telskyu Hong, DongHyun Kim, Mingj Ji, Wonseek Hwang, Daehyun Nam, and Sungra Park. 2022. BROS: A Pre-Trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. In _AAAI_.",
        "Huang et al. (2021) Yuyan Huang, Hongwei Xue, Bi Liu, and Yutong Tu. 2021. Unifying multimodal transformer for bi-directional image and text generation. In _ACM Multimedia_.",
        "Huang et al. (2021) Zhicheng Huang, Zhuoyang Zeng, Yupan Huang, Bie Liu, Dongmin Tu, and Jianlong Fu. 2021. Seeing out of the Deep: End-to-end pre-training for vision-language representation learning. In _CVPR_.",
        "Jaume et al. (2019) Guillaume Jaume, Harmi Kemal Elencel, and Jean-Philippe Thiran. 2019. Funsd: A dataset for form understanding in noisy scanned documents. In _ICDARW_.",
        "Joshi et al. (2020) Mandar Joshi, Dangi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Snapnet: Improving pre-training by representing and predicting spans. _Transactions of the Association for Computational Linguistics_ (2020), 64-77.",
        "Kim et al. (2021) Wonjae Kim, Bokyung Son, and Idoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In _ICML_.",
        "Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_ (2014).",
        "Lample et al. (2016) Guillaume Lample, Miguel Ballesteros, Sande Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In _NAACL Intl_.",
        "Lee et al. (2022) Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, and Tomas Pfister. 2022. FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. In _ACL_.",
        "Lewis et al. (2006) D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. 2006. Building a Test Collection for Complex Document Information Processing. In _SIGIR_."
    ]
}