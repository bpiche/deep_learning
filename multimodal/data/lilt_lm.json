{
    "Title": "LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding",
    "Authors": "Jiapeng Wang\\({}^{1}\\)   Lianwen Jin\\({}^{*1,3,4}\\)   Kai Ding\\({}^{*2,3}\\)\n\n\\({}^{1}\\)South China University of Technology, Guangzhou, China\n\n\\({}^{2}\\)IntSig Information Co., Ltd, Shanghai, China\n\n\\({}^{3}\\)INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding, China\n\n\\({}^{4}\\)Peng Cheng Laboratory, Shenzhen, China\n\n\\({}^{1}\\)eejpwang@mail.scut.edu.cn, eelwjin@scut.edu.cn\n\n\\({}^{2}\\)danny_ding@intsig.net\n\nCorresponding author.",
    "Abstract": "Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective **L**anguage-**i**ndependent **L**ayout **T**ransformer (**LiLT**) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.",
    "Key Findings": "In this paper, we present LiLT, a language-independent layout Transformer that can learn the layout knowledge from monolingual structured documents and then generalize it to deal with multilingual ones. Our framework successfully first decouples the text and layout information in pre-training and then re-couples them for fine-tuning. Experimental results on eight languages under three settings (language-specific, cross-lingual zero-shot transfer, and multi-task fine-tuning) have fully illustrated its effectiveness, which substantially bridges the language gap in real-world structured document understanding applications. The public availability of LiLT is also expected to promote the development of document intelligence.",
    "References": [
        "M. Z. Afzal, A. Kolsch, S. Ahmed, and M. Liwicki (2017)Cutting the error by half: investigation of very deep cnn and advanced training strategies for document image classification. In ICDAR, Vol. 1, pp. 883-888. Cited by: SS1.",
        "S. Appalaraju, B. Jasani, B. U. Kota, Y. Xie, and R. Manmatha (2021)DocFormer: end-to-end transformer for document understanding. In ICCV, Cited by: SS1.",
        "D. A. Borges Oliveira et al. (2017)Fast cnn-based document layout analysis. In ICCV Workshop, pp. 1173-1180. Cited by: SS1.",
        "J. E. Ba, J. R. Kiros, and G. E. Hinton (2016)Layer normalization. arXiv preprint arXiv:1607.06450. Cited by: SS1.",
        "H. Bao, L. Dong, F. Wei, W. Wang, N. Yang, X. Liu, Y. Wang, J. Gao, S. Piao, M. Zhou, et al. (2020)UniLMv2: pseudo-masked language models for unified language model pre-training. In ICML, pp. 642-652. Cited by: SS1.",
        "Z. Chi, L. Dong, F. Wei, N. Yang, S. Singhal, W. Wang, X. Song, X. Mao, H. Huang, and M. Zhou (2021)InfoXLM: an information-theoretic framework for cross-lingual language model pre-training. In NAACL-HLT, pp. 3576-3588. Cited by: SS1.",
        "A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov (2020)Unsupervised cross-lingual representation learning at scale. In ACL, pp. 8440-8451. Cited by: SS1.",
        "Y. Cui, W. Che, T. Liu, B. Qin, S. Wang, and G. Hu (2020)Revisiting pre-trained models for Chinese natural language processing. In Findings of EMNLP, pp. 657-668. Cited by: SS1.",
        "A. Das, S. Roy, U. Bhattacharya, and S. K. Parui (2018)Document image classification with intra-domain transfer learning and stacked generalization of deep convolutional neural networks. In ICPR, pp. 3180-3185. Cited by: SS1.",
        "T. Dauphinee, N. Patel, and M. Rashidi (2019)Modular multimodal architecture for document classification. arXiv preprint arXiv:1912.04376. Cited by: SS1.",
        "T. I. Denk and C. Reisswig (2019)BERTgrid: contextualized embedding for 2d document representation and understanding. In Workshop on Document Intelligence at NeurIPS, Cited by: SS1.",
        "J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019)BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pp. 4171-4186. Cited by: SS1.",
        "L. Garncarek, R. Powalski, T. Stanislawek, B. Topolski, P. Halama, and F. Gralinski (2021)LAMBERT: layout-aware (language) modeling using BERT for information extraction. In ICDAR, Cited by: SS1.",
        "A. W. Harley et al. (2015)Evaluation of deep convolutional nets for document image classification and retrieval. In ICDAR, pp. 991-995. Cited by: SS1.",
        "T. Hong, D. Kim, M. Ji, W. Hwang, D. Nam, and S. Park (2020)BROS: a pre-trained language model for understanding texts in document. Cited by: SS1.",
        "G. Jaume et al. (2019)FUNSD: a dataset for form understanding in noisy scanned documents. In ICDAR, Vol. 2, pp. 1-6. Cited by: SS1.",
        "A. R. Katti, C. Reisswig, C. Guder, S. B. Breda, S. Bickel, J. Hohne, and J. Faddoul (2018)Chargrid: towards understanding 2d documents. In EMNLP, pp. 4459-4469. Cited by: SS1.",
        "D. P. Kingma and J. Ba (2015)Adam: a method for stochastic optimization. In ICLR, Cited by: SS4.1.",
        "G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer (2016)Neural architectures for named entity recognition. In NAACL-HLT, pp. 260-270. Cited by: SS1.",
        "D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard (2006)Building a test collection for complex document information processing. In ACM SIGIR, pp. 665-666. Cited by: SS1.",
        "C. Li, B. Bi, M. Yan, W. Wang, S. Huang, F. Huang, and L. Si (2021)StructuralLM: structural pre-training for form understanding. In ACL, Cited by: SS1.",
        "P. Li, J. Gu, J. Kuen, V. I. Morariu, H. Zhao, R. Jain, V. Manjunatha, and H. Liu (2021)SelfDoc: self-supervised document representation learning. In CVPR, pp. 5652-5660. Cited by: SS1.",
        "Y. Li, Y. Qian, Y. Yu, X. Qin, C. Zhang, Y. Liu, K. Yao, J. Han, J. Liu, and E. Ding (2021)StrucTexT: structured text understanding with multi-modal transformers. In ACM-MM, Cited by: SS1."
    ]
}